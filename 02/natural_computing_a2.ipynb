{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural computing - Assignment 2\n",
    "\n",
    "#### Jelle Arts (s1010317), Ruben Geurtjens (s1006223), Lotte Willems (s1009251)\n",
    "\n",
    "\n",
    "The link to our git where you can find the notebook: https://github.com/stra3/NaturalComputing_assignments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1\n",
    "\n",
    "\n",
    "* Fitness particle $x_1$: $f(x_1) = \\sum_{i=1}^{2} \\space (--400) \\cdot \\sin(\\sqrt|-400|) = 730.356$\n",
    "\n",
    "    Fitness particle $x_2$: $f(x_2) = \\sum_{i=1}^{2} \\space (--410) \\cdot \\sin(\\sqrt|-410|) = 807.915$ \n",
    "    \n",
    "    Fitness particle $x_3$: $f(x_3) = \\sum_{i=1}^{2} \\space (--415) \\cdot \\sin(\\sqrt|-415|) = 829.012$\n",
    "    \n",
    "\n",
    "* Particle updates:\n",
    "    $v_i = \\omega v_i + \\alpha_1 r_1 (x_{i}^{*} - x_i) + \\alpha_2 r_2(x^{*} - x_i)$\n",
    "    \n",
    "    **For $\\omega = 2$:**\n",
    "\n",
    "    Velocity particle $x_1$: $v_1 = 2 \\cdot -50 + 1 \\cdot 0.5 (-400 - -400) + 1 \\cdot 0.5 (-415 - -400) = -107.5 $\n",
    "    \n",
    "    New position particle $x_1$: $(-400, -400) + (-107.5, -107,5) = (-507.5, -507.5)$\n",
    "    \n",
    "    Fitness particle $x_1$: $f(x_1) = \\sum_{i=1}^{2} \\space (--507.5) \\cdot \\sin(\\sqrt|-507.5|) = -518.896$\n",
    "    \n",
    "    Velocity particle $x_2$: $v_2 = 2 \\cdot -50 + 1 \\cdot 0.5 (-410 - -410) + 1 \\cdot 0.5 (-415 - -410) = -102.5 $\n",
    "    \n",
    "    New position particle $x_2$: $(-410, -410) + (-102.5, -102.5) = (-512.5, -512.5)$ \n",
    "    \n",
    "    Fitness particle $x_2$: $f(x_2) = \\sum_{i=1}^{2} \\space (--512.5) \\cdot \\sin(\\sqrt|-512.5|) = -618.122 $\n",
    "    \n",
    "    Velocity particle $x_3$: $v_3 = 2 \\cdot -50 + 1 \\cdot 0.5 (-415 - -415) + 1 \\cdot 0.5 (-415 - -415) = -100$\n",
    "    \n",
    "    New position particle $x_3$: $(-415, 415) + (-100, -100) = (-515, -515)$\n",
    "    \n",
    "    Fitness particle $x_3$: $f(x_3) = \\sum_{i=1}^{2} \\space (--515) \\cdot \\sin(\\sqrt|-515|) = -665.482$\n",
    "    \n",
    "    **For $\\omega = 0.5$:**\n",
    "    \n",
    "    Velocity particle $x_1$: $v_1 = 0.5 \\cdot -50 + 1 \\cdot 0.5 (-400 - -400) + 1 \\cdot 0.5 (-415 - -400) = -32.5 $\n",
    "    \n",
    "    New position particle $x_1$: $(-400, -400) + (-32.5, -32.5) = (-432.5, -432.5)$\n",
    "    \n",
    "    Fitness particle $x_1$: $f(x_1) = \\sum_{i=1}^{2} \\space (--432.5) \\cdot \\sin(\\sqrt|-432.5|) = 804.482$\n",
    "    \n",
    "    Velocity particle $x_2$: $v_2 = 0.5 \\cdot -50 + 1 \\cdot 0.5 (-410 - -410) + 1 \\cdot 0.5 (-415 - -410) = -27.5 $\n",
    "    \n",
    "    New position particle $x_2$: $(-410, -410) + (-27.5, -27.5) = (-437.5, -437.5)$ \n",
    "    \n",
    "    Fitness particle $x_2$: $f(x_2) = \\sum_{i=1}^{2} \\space (--437.5) \\cdot \\sin(\\sqrt|-437.5|) = 796.495 $\n",
    "    \n",
    "    Velocity particle $x_3$: $v_3 = 0.5 \\cdot -50 + 1 \\cdot 0.5 (-415 - -415) + 1 \\cdot 0.5 (-415 - -415) = -25$\n",
    "    \n",
    "    New position particle $x_3$: $(-415, 415) + (-25, -25) = (-440, -440)$\n",
    "    \n",
    "    Fitness particle $x_3$: $f(x_3) = \\sum_{i=1}^{2} \\space (--440) \\cdot \\sin(\\sqrt|-440|) = 747.530$\n",
    "    \n",
    "    **For $\\omega = 0.1$:**\n",
    "    \n",
    "    Velocity particle $x_1$: $v_1 = 0.1 \\cdot -50 + 1 \\cdot 0.5 (-400 - -400) + 1 \\cdot 0.5 (-415 - -400) = -12.5 $\n",
    "    \n",
    "    New position particle $x_1$: $(-400, -400) + (-12.5, -12.5) = (-412.5, -412.5)$\n",
    "    \n",
    "    Fitness particle $x_1$: $f(x_1) = \\sum_{i=1}^{2} \\space (--412.5) \\cdot \\sin(\\sqrt|-412.5|) = 819.991$\n",
    "    \n",
    "    Velocity particle $x_2$: $v_2 = 0.1 \\cdot -50 + 1 \\cdot 0.5 (-410 - -410) + 1 \\cdot 0.5 (-415 - -410) = -7.5 $\n",
    "    \n",
    "    New position particle $x_2$: $(-410, -410) + (-7.5, -7.5) = (-417.5, -417.5)$ \n",
    "    \n",
    "    Fitness particle $x_2$: $f(x_2) = \\sum_{i=1}^{2} \\space (--417.5) \\cdot \\sin(\\sqrt|-417.5|) = 834.935 $\n",
    "    \n",
    "    Velocity particle $x_3$: $v_3 = 0.1 \\cdot -50 + 1 \\cdot 0.5 (-415 - -415) + 1 \\cdot 0.5 (-415 - -415) = -5$\n",
    "    \n",
    "    New position particle $x_3$: $(-415, 415) + (-5, -5) = (-420, -420)$\n",
    "    \n",
    "    Fitness particle $x_3$: $f(x_3) = \\sum_{i=1}^{2} \\space (--420) \\cdot \\sin(\\sqrt|-420|) = 837.729$\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "* Parameter $\\omega$ is the factor that determines how much the velocity of the previous position of the particle will contribute to the next velocity. From the lecture, the parameter influences the convergence of the swarm. With $\\omega > 1$, velocities increase over time causing divergent behaviour causing particles failing to change direction and restraining them from moving back towards promising areas. With $\\omega < 1$, the particles can decelerate until the velocities reach zero.\n",
    "\n",
    "* An advantage of a high $\\omega$ is that it allows the particles to move faster to the search space allowing them to explore and find a better solution. A disadvantage is that there is less exploitation, and it might therefore miss the global optimum.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2\n",
    "\n",
    "With an $\\omega < 1$, the velocity will eventually reach 0. At the start, the velocity is pointing away from the optimum. However, since the velocity decreases each step, it will move back, towards the optimum, before it reaches 0. If the particle reaches the optimum before the velocity reaches zero, the algorithm will terminate at the optimum point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.cluster import KMeans\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantization_error(clusters, centroids):\n",
    "    \"\"\"\n",
    "    \n",
    "    cluster: clusters to calculate error for\n",
    "    centroids: centroids of the particles\n",
    "    returns: quantization error for clustering\n",
    "    \n",
    "    \"\"\"\n",
    "    n_c = len(clusters)\n",
    "    \n",
    "    quant_error = 0.0\n",
    "    for cluster, centroid in zip(clusters, centroids):\n",
    "        euclidean_sum = np.sum([scipy.spatial.distance.euclidean(x, centroid) for x in cluster])\n",
    "        if len(cluster) == 0:\n",
    "            quant_error += np.inf\n",
    "        else:\n",
    "            quant_error += euclidean_sum/len(cluster)\n",
    "    \n",
    "    quant_error /= n_c\n",
    "    \n",
    "    return quant_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Artificial dataset 1 (from paper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Creating artificial dataset 1 as given in the paper\n",
    "z1 = np.random.uniform(-1, 1, 400)\n",
    "z2 = np.random.uniform(-1, 1, 400)\n",
    "label = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>z1</th>\n",
       "      <th>z2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.181227</td>\n",
       "      <td>0.950388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.211210</td>\n",
       "      <td>0.340246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.809732</td>\n",
       "      <td>0.055411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.230850</td>\n",
       "      <td>-0.479400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.207738</td>\n",
       "      <td>0.524653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395</th>\n",
       "      <td>-0.455079</td>\n",
       "      <td>0.342603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>-0.039034</td>\n",
       "      <td>0.381581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397</th>\n",
       "      <td>0.069812</td>\n",
       "      <td>0.045605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>398</th>\n",
       "      <td>-0.203490</td>\n",
       "      <td>0.666257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399</th>\n",
       "      <td>-0.023589</td>\n",
       "      <td>-0.495622</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>400 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           z1        z2\n",
       "0   -0.181227  0.950388\n",
       "1   -0.211210  0.340246\n",
       "2    0.809732  0.055411\n",
       "3    0.230850 -0.479400\n",
       "4   -0.207738  0.524653\n",
       "..        ...       ...\n",
       "395 -0.455079  0.342603\n",
       "396 -0.039034  0.381581\n",
       "397  0.069812  0.045605\n",
       "398 -0.203490  0.666257\n",
       "399 -0.023589 -0.495622\n",
       "\n",
       "[400 rows x 2 columns]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "artificial_ds = pd.DataFrame({\"z1\" : z1, \"z2\" : z2})\n",
    "artificial_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Iris dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Loading in iris dataset\n",
    "iris_data = datasets.load_iris()\n",
    "\n",
    "iris_ds = pd.DataFrame(iris_data.data, columns = iris_data.feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K Means artificial dataset 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Running K Means on artificial dataset 1\n",
    "\n",
    "kmeans_art = KMeans(n_clusters = 2).fit(artificial_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_clusters_art = kmeans_art.predict(artificial_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clusters(pred, ds):\n",
    "    \"\"\"\n",
    "    \n",
    "    pred: predicted class by clustering method\n",
    "    ds: dataset\n",
    "    returns: list of clusters for datapoints from dataset\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    cluster_0 = []\n",
    "    cluster_1 = []\n",
    "\n",
    "    for i in range(0, len(ds)):\n",
    "        if pred[i] == 0:\n",
    "            cluster_0.append(ds.loc[i])\n",
    "\n",
    "        if pred[i] == 1:\n",
    "            cluster_1.append(ds.loc[i])\n",
    "            \n",
    "    return [cluster_0, cluster_1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantization error artificial dataset 1 data KMeans 0.5915534283523592\n"
     ]
    }
   ],
   "source": [
    "clusters_art = get_clusters(predicted_clusters_art, artificial_ds)\n",
    "error_a = quantization_error(clusters_art, kmeans_art.cluster_centers_)\n",
    "\n",
    "print(\"Quantization error artificial dataset 1 data KMeans\", error_a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K Means iris datset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_iris = KMeans(n_clusters = 3).fit(iris_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_clusters_iris = kmeans_iris.predict(iris_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clusters_iris(pred, ds):\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    pred: predicted class by clustering method\n",
    "    ds: dataset\n",
    "    returns: list of clusters for datapoints from dataset\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    cluster_0 = []\n",
    "    cluster_1 = []\n",
    "    cluster_2 = []\n",
    "\n",
    "    for i in range(0, len(ds)):\n",
    "        if pred[i] == 0:\n",
    "            cluster_0.append(ds.loc[i])\n",
    "\n",
    "        if pred[i] == 1:\n",
    "            cluster_1.append(ds.loc[i])\n",
    "            \n",
    "        if pred[i] == 2:\n",
    "            cluster_2.append(ds.loc[i])\n",
    "            \n",
    "    return [cluster_0, cluster_1, cluster_2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantization error iris data KMeans 0.6465653848597094\n"
     ]
    }
   ],
   "source": [
    "clusters_iris = get_clusters_iris(predicted_clusters_iris, iris_ds)\n",
    "error_i = quantization_error(clusters_iris, kmeans_iris.cluster_centers_)\n",
    "\n",
    "print(\"Quantization error iris data KMeans\", error_i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PSO algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_velocity(particle, velocity, personal_best_pos, global_best_pos):\n",
    "    \n",
    "    \"\"\"\n",
    "    update rule as given in slides\n",
    "    \n",
    "    particle: particle to update pos of\n",
    "    velocity: used to update particle pos\n",
    "    personal_best_pos: best pos of the particle\n",
    "    global_best_pos: best pos\n",
    "    returns new position of the particle and new velocity\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    omega = 0.72\n",
    "    alpha = 1.49\n",
    "\n",
    "    particle = np.array(particle)\n",
    "\n",
    "    r1 = np.random.uniform(0, 1)\n",
    "    r2 = np.random.uniform(0, 1)\n",
    "    \n",
    "    inertia = omega * velocity\n",
    "\n",
    "    personal_infl = alpha * r1 * (personal_best_pos - particle)\n",
    "\n",
    "    social_infl = alpha * r2 * (global_best_pos - particle)\n",
    "\n",
    "    velocity = inertia + personal_infl + social_infl\n",
    "\n",
    "    particle = particle + velocity\n",
    "                \n",
    "    return velocity, particle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_particles_ds1(n_particles, n_centroids, ds):\n",
    "    \"\"\"\n",
    "    n_particles: number of particles to generate\n",
    "    n_centroids: nr of centroids to generate per particle\n",
    "    ds: dataset to get centroids from\n",
    "    returns: list of particles and centroids\n",
    "    \n",
    "    \"\"\"\n",
    "    particles = []\n",
    "    for i in range(0, n_particles):\n",
    "        centroids = []\n",
    "        for j in range(0, n_centroids):\n",
    "            centroids.append([ds.iloc[np.random.randint(0, len(ds))][\"z1\"], ds.iloc[np.random.randint(0, len(ds))][\"z2\"]])\n",
    "        particles.append(centroids)\n",
    "    return particles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pso(particles, ds, n_clusters, n_iters):\n",
    "    \"\"\"\n",
    "    particles: list of particles and their centroids\n",
    "    ds: dataset to apply pso on\n",
    "    n_iters: nr of iterations\n",
    "    returns: global best error and global best position\n",
    "    \"\"\"\n",
    "    ds = ds.to_numpy()\n",
    "    \n",
    "    global_best_fitness = float(\"-inf\")\n",
    "    global_best_error = float(\"inf\")\n",
    "    global_best_pos = []\n",
    "    personal_best_error = np.full((len(particles)), np.inf)\n",
    "    personal_best_pos = np.zeros((len(particles), len(particles[0]), len(ds[0])))\n",
    "    velocity = np.zeros((len(particles), len(particles[0]), len(ds[0])))\n",
    "    \n",
    "    min_quant_error = []\n",
    "    \n",
    "    for i in range(0, n_iters):\n",
    "        # for each iteration, assign datapoints to cluster based on particle centroids \n",
    "        # and compute quantization error\n",
    "        \n",
    "        quantization_errors = []\n",
    "        \n",
    "        for particle in particles:\n",
    "            \n",
    "            clusters = [[] for i in range(n_clusters)]\n",
    "                        \n",
    "            for datapoint in ds:\n",
    "                min_dist = float(\"inf\")\n",
    "                cluster_idx = None\n",
    "                \n",
    "                for c in range(0, n_clusters):\n",
    "                    dist = scipy.spatial.distance.euclidean(datapoint, particle[c])\n",
    "                    if dist < min_dist:\n",
    "                        min_dist = dist\n",
    "                        cluster_idx = c\n",
    "                        \n",
    "                clusters[cluster_idx].append(datapoint)\n",
    "                \n",
    "            error = quantization_error(clusters, particle)        \n",
    "            quantization_errors.append(error)\n",
    "            \n",
    "        # keeping track of min quantization error per iteration \n",
    "        min_quant_error.append(np.min(quantization_errors))\n",
    "        \n",
    "        # updating best error and position for update rule\n",
    "        for idx, q_error in enumerate(quantization_errors):\n",
    "            \n",
    "            if q_error < personal_best_error[idx]:\n",
    "                personal_best_error[idx] = q_error\n",
    "                personal_best_pos[idx] = particles[idx]\n",
    "            \n",
    "            if personal_best_error[idx] < global_best_error:\n",
    "                global_best_error = personal_best_error[idx]\n",
    "                global_best_pos = particles[idx]\n",
    "                \n",
    "        \n",
    "        # updating the velocity and particle position based on update rule from slides\n",
    "        for idx, particle in enumerate(particles):\n",
    "            velocity[idx], particles[idx] = update_velocity(particle, velocity[idx], personal_best_pos[idx], global_best_pos)\n",
    "            \n",
    "    return global_best_error, global_best_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "particles_ds1 = generate_particles_ds1(10, 2, artificial_ds)\n",
    "\n",
    "global_best_error_ds1, global_best_pos_ds1 = pso(particles_ds1, artificial_ds, n_clusters = 2, n_iters = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global best error artificial dataset 1: 0.5559434506619045\n"
     ]
    }
   ],
   "source": [
    "print(\"Global best error artificial dataset 1:\", global_best_error_ds1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global best position artificial dataset 1: [[0.7245862  0.78570082]\n",
      " [0.27788548 0.35389061]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Global best position artificial dataset 1:\", global_best_pos_ds1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_particles_iris(n_particles, n_centroids, ds):\n",
    "    \"\"\"\n",
    "    \n",
    "    n_particles: number of particles to generate\n",
    "    n_centroids: nr of centroids to generate per particle\n",
    "    ds: dataset to get centroids from\n",
    "    returns: list of particles and centroids\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    particles = []\n",
    "    for i in range(0, n_particles):\n",
    "        centroids = []\n",
    "        for j in range(0, n_centroids):\n",
    "            centroids.append([ds.iloc[np.random.randint(0, len(ds))][\"sepal length (cm)\"], \n",
    "                              ds.iloc[np.random.randint(0, len(ds))][\"sepal width (cm)\"],\n",
    "                              ds.iloc[np.random.randint(0, len(ds))][\"petal length (cm)\"],\n",
    "                              ds.iloc[np.random.randint(0, len(ds))][\"petal width (cm)\"],\n",
    "                             ])\n",
    "        particles.append(centroids)\n",
    "    return particles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "particles_iris = generate_particles_iris(10, 3, iris_ds)\n",
    "\n",
    "global_best_error_iris, global_best_pos_iris = pso(particles_iris, iris_ds, n_clusters = 3, n_iters = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global best error for iris data using pso: 0.8339814395709828\n"
     ]
    }
   ],
   "source": [
    "print(\"Global best error for iris data using pso:\", global_best_error_iris)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global best position irs data: [[5.97416615 2.41215741 4.39441849 1.12423999]\n",
      " [4.74590116 2.43523205 3.21123253 0.89802293]\n",
      " [5.50354652 3.08352869 1.49882136 0.13911237]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Global best position irs data:\", global_best_pos_iris)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparing KMeans and PSO\n",
    "\n",
    "The results from implementing KMeans and PSO on the artificial dataset 1 and the iris dataset shows us that PSO performs slightly better compared to KMeans. However, our implementation of PSO on the iris dataset is not always performing better.\n",
    "From literature it shows that PSO is supposed to be slightly better than KMeans."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4\n",
    "\n",
    "* ACO is not a CBS for this problem as some edges (components) can occur in more feasible solutions. In the given problem this can happen with the middel 2 edges, as these can be included in multiple solutions. \n",
    "\n",
    "* The induced bias is not always harmful. If the bias points towards an optimal solution, it can enhance finding the optimal solution. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ACO (ant colony optimization) is an algorithm that is based on the foraging behaviour of ants. More specifically, it is a probablistic algorithm that can be used for path finding. Ants are able to solve challeging path finding problems with the use of pheromones. They are able to do this by basing their routing on the concentration of pheromone, picking the paths with a higher concentration of pheromones more often. This can be represented in a graph where the nodes are solution components and the edge weights are pheromone values. The goal is to reach the destination node following the shortest path in the graph for which the ACO algorithm can be used. In an ACO without a tabu list, ants will make cycles in the graph because they do not know they already visited that path. In the given figure, this will occur almost definitely in the bottom part since there are a lot of cycles possible, resulting in a suboptimal solution. Using tabu list, this can be prevented. Tabu uses memory, which enables the algorithm to remember moves and solutions that are already exploited.\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
