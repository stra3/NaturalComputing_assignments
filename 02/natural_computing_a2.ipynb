{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural computing - Assignment 2\n",
    "\n",
    "#### Jelle Arts (s1010317), Ruben Geurtjens (s1006223), Lotte Willems (s1009251)\n",
    "\n",
    "\n",
    "The link to our git where you can find the notebook:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1\n",
    "\n",
    "\n",
    "* Fitness particle $x_1$: $f(x_1) = \\sum_{i=1}^{2} \\space (--400) \\cdot \\sin(\\sqrt|-400|) = 730.356$\n",
    "\n",
    "    Fitness particle $x_2$: $f(x_2) = \\sum_{i=1}^{2} \\space (--410) \\cdot \\sin(\\sqrt|-410|) = 807.915$ \n",
    "    \n",
    "    Fitness particle $x_3$: $f(x_3) = \\sum_{i=1}^{2} \\space (--415) \\cdot \\sin(\\sqrt|-415|) = 829.012$\n",
    "    \n",
    "\n",
    "* Particle updates:\n",
    "    $v_i = \\omega v_i + \\alpha_1 r_1 (x_{i}^{*} - x_i) + \\alpha_2 r_2(x^{*} - x_i)$\n",
    "    \n",
    "    **For $\\omega = 2$:**\n",
    "\n",
    "    Velocity particle $x_1$: $v_1 = 2 \\cdot -50 + 1 \\cdot 0.5 (-400 - -400) + 1 \\cdot 0.5 (-415 - -400) = -107.5 $\n",
    "    \n",
    "    New position particle $x_1$: $(-400, -400) + (-107.5, -107,5) = (-507.5, -507.5)$\n",
    "    \n",
    "    Fitness particle $x_1$: $f(x_1) = \\sum_{i=1}^{2} \\space (--507.5) \\cdot \\sin(\\sqrt|-507.5|) = -518.896$\n",
    "    \n",
    "    Velocity particle $x_2$: $v_2 = 2 \\cdot -50 + 1 \\cdot 0.5 (-410 - -410) + 1 \\cdot 0.5 (-415 - -410) = -102.5 $\n",
    "    \n",
    "    New position particle $x_2$: $(-410, -410) + (-102.5, -102.5) = (-512.5, -512.5)$ \n",
    "    \n",
    "    Fitness particle $x_2$: $f(x_2) = \\sum_{i=1}^{2} \\space (--512.5) \\cdot \\sin(\\sqrt|-512.5|) = -618.122 $\n",
    "    \n",
    "    Velocity particle $x_3$: $v_3 = 2 \\cdot -50 + 1 \\cdot 0.5 (-415 - -415) + 1 \\cdot 0.5 (-415 - -415) = -100$\n",
    "    \n",
    "    New position particle $x_3$: $(-415, 415) + (-100, -100) = (-515, -515)$\n",
    "    \n",
    "    Fitness particle $x_3$: $f(x_3) = \\sum_{i=1}^{2} \\space (--515) \\cdot \\sin(\\sqrt|-515|) = -665.482$\n",
    "    \n",
    "    **For $\\omega = 0.5$:**\n",
    "    \n",
    "    Velocity particle $x_1$: $v_1 = 0.5 \\cdot -50 + 1 \\cdot 0.5 (-400 - -400) + 1 \\cdot 0.5 (-415 - -400) = -32.5 $\n",
    "    \n",
    "    New position particle $x_1$: $(-400, -400) + (-32.5, -32.5) = (-432.5, -432.5)$\n",
    "    \n",
    "    Fitness particle $x_1$: $f(x_1) = \\sum_{i=1}^{2} \\space (--432.5) \\cdot \\sin(\\sqrt|-432.5|) = 804.482$\n",
    "    \n",
    "    Velocity particle $x_2$: $v_2 = 0.5 \\cdot -50 + 1 \\cdot 0.5 (-410 - -410) + 1 \\cdot 0.5 (-415 - -410) = -27.5 $\n",
    "    \n",
    "    New position particle $x_2$: $(-410, -410) + (-27.5, -27.5) = (-437.5, -437.5)$ \n",
    "    \n",
    "    Fitness particle $x_2$: $f(x_2) = \\sum_{i=1}^{2} \\space (--437.5) \\cdot \\sin(\\sqrt|-437.5|) = 796.495 $\n",
    "    \n",
    "    Velocity particle $x_3$: $v_3 = 0.5 \\cdot -50 + 1 \\cdot 0.5 (-415 - -415) + 1 \\cdot 0.5 (-415 - -415) = -25$\n",
    "    \n",
    "    New position particle $x_3$: $(-415, 415) + (-25, -25) = (-440, -440)$\n",
    "    \n",
    "    Fitness particle $x_3$: $f(x_3) = \\sum_{i=1}^{2} \\space (--440) \\cdot \\sin(\\sqrt|-440|) = 747.530$\n",
    "    \n",
    "    **For $\\omega = 0.1$:**\n",
    "    \n",
    "    Velocity particle $x_1$: $v_1 = 0.1 \\cdot -50 + 1 \\cdot 0.5 (-400 - -400) + 1 \\cdot 0.5 (-415 - -400) = -12.5 $\n",
    "    \n",
    "    New position particle $x_1$: $(-400, -400) + (-12.5, -12.5) = (-412.5, -412.5)$\n",
    "    \n",
    "    Fitness particle $x_1$: $f(x_1) = \\sum_{i=1}^{2} \\space (--412.5) \\cdot \\sin(\\sqrt|-412.5|) = 819.991$\n",
    "    \n",
    "    Velocity particle $x_2$: $v_2 = 0.1 \\cdot -50 + 1 \\cdot 0.5 (-410 - -410) + 1 \\cdot 0.5 (-415 - -410) = -7.5 $\n",
    "    \n",
    "    New position particle $x_2$: $(-410, -410) + (-7.5, -7.5) = (-417.5, -417.5)$ \n",
    "    \n",
    "    Fitness particle $x_2$: $f(x_2) = \\sum_{i=1}^{2} \\space (--417.5) \\cdot \\sin(\\sqrt|-417.5|) = 834.935 $\n",
    "    \n",
    "    Velocity particle $x_3$: $v_3 = 0.1 \\cdot -50 + 1 \\cdot 0.5 (-415 - -415) + 1 \\cdot 0.5 (-415 - -415) = -5$\n",
    "    \n",
    "    New position particle $x_3$: $(-415, 415) + (-5, -5) = (-420, -420)$\n",
    "    \n",
    "    Fitness particle $x_3$: $f(x_3) = \\sum_{i=1}^{2} \\space (--420) \\cdot \\sin(\\sqrt|-420|) = 837.729$\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "* Parameter $\\omega$ is the factor that determines how much the velocity of the previous position of the particle will contribute to the next velocity. From the lecture, the parameter influences the convergence of the swarm. With $\\omega > 1$, velocities increase over time causing divergent behaviour causing particles failing to change direction and restraining them from moving back towards promising areas. With $\\omega < 1$, the particles can decelerate until the velocities reach zero.\n",
    "\n",
    "* An advantage of a high $\\omega$ is that it allows the particles to move faster to the search space allowing them to explore and find a better solution. A disadvantage is that there is less exploitation, and it might therefore miss the global optimum.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2\n",
    "\n",
    "With an $\\omega < 1$, the velocity will eventually reach 0. At the start, the velocity is pointing away from the optimum. However, since the velocity decreases each step, it will move back, towards the optimum, before it reaches 0. If the particle reaches the optimum before the velocity reaches zero, the algorithm will terminate at the optimum point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.cluster import KMeans\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantization_error(clusters, centroids):\n",
    "    n_c = len(clusters)\n",
    "    \n",
    "    quant_error = 0.0\n",
    "    for cluster, centroid in zip(clusters, centroids):\n",
    "        euclidean_sum = np.sum([np.linalg.norm(x - centroid) for x in cluster])\n",
    "        quant_error += euclidean_sum/len(cluster)\n",
    "        \n",
    "    quant_error /= n_c\n",
    "    \n",
    "    return quant_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Artificial dataset 1 (from paper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "z1 = np.random.uniform(-1, 1, 400)\n",
    "z2 = np.random.uniform(-1, 1, 400)\n",
    "label = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(0, 400):\n",
    "#     if (z1[i] >= 0.7) or ((z1[i] <= 0.3) and (z2[i] >= (-0.2 - z1[i]))):\n",
    "#         label.append(1)\n",
    "        \n",
    "#     else:\n",
    "#         label.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>z1</th>\n",
       "      <th>z2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.705926</td>\n",
       "      <td>0.030330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.105512</td>\n",
       "      <td>-0.252670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.738625</td>\n",
       "      <td>0.236769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.445644</td>\n",
       "      <td>0.501340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.730516</td>\n",
       "      <td>0.895779</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         z1        z2\n",
       "0  0.705926  0.030330\n",
       "1  0.105512 -0.252670\n",
       "2  0.738625  0.236769\n",
       "3 -0.445644  0.501340\n",
       "4 -0.730516  0.895779"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "artificial_ds = pd.DataFrame({\"z1\" : z1, \"z2\" : z2})\n",
    "artificial_ds.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Iris dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length (cm)</th>\n",
       "      <th>sepal width (cm)</th>\n",
       "      <th>petal length (cm)</th>\n",
       "      <th>petal width (cm)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)\n",
       "0                5.1               3.5                1.4               0.2\n",
       "1                4.9               3.0                1.4               0.2\n",
       "2                4.7               3.2                1.3               0.2\n",
       "3                4.6               3.1                1.5               0.2\n",
       "4                5.0               3.6                1.4               0.2"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris_data = datasets.load_iris()\n",
    "\n",
    "iris_ds = pd.DataFrame(iris_data.data, columns = iris_data.feature_names)\n",
    "\n",
    "iris_ds.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K Means artificial dataset 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_art = KMeans(n_clusters = 2).fit(artificial_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_clusters_art = kmeans_art.predict(artificial_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clusters(pred, ds):\n",
    "    \n",
    "    cluster_0 = []\n",
    "    cluster_1 = []\n",
    "\n",
    "    for i in range(0, len(ds)):\n",
    "        if pred[i] == 0:\n",
    "            cluster_0.append(ds.loc[i])\n",
    "\n",
    "        if pred[i] == 1:\n",
    "            cluster_1.append(ds.loc[i])\n",
    "    return [cluster_0, cluster_1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.598341835897126"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clusters_art = get_clusters(predicted_clusters_art, artificial_ds)\n",
    "quantization_error(clusters_art, kmeans_art.cluster_centers_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K Means iris datset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_iris = KMeans(n_clusters = 3).fit(iris_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_clusters_iris = kmeans_iris.predict(iris_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clusters_iris(pred, ds):\n",
    "    \n",
    "    cluster_0 = []\n",
    "    cluster_1 = []\n",
    "    cluster_2 = []\n",
    "\n",
    "    for i in range(0, len(ds)):\n",
    "        if pred[i] == 0:\n",
    "            cluster_0.append(ds.loc[i])\n",
    "\n",
    "        if pred[i] == 1:\n",
    "            cluster_1.append(ds.loc[i])\n",
    "            \n",
    "        if pred[i] == 2:\n",
    "            cluster_2.append(ds.loc[i])\n",
    "            \n",
    "    return [cluster_0, cluster_1, cluster_2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6465653848597094"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clusters_iris = get_clusters_iris(predicted_clusters_iris, iris_ds)\n",
    "quantization_error(clusters_iris, kmeans_iris.cluster_centers_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PSO algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_particles(n_particles, n_centroids, ds):\n",
    "    particles = []\n",
    "    for i in range(0, n_particles):\n",
    "        centroids = []\n",
    "        for j in range(0, n_centroids):\n",
    "            centroids.append([ds.iloc[np.random.randint(0, len(ds))][\"z1\"], ds.iloc[np.random.randint(0, len(ds))][\"z2\"]])\n",
    "        particles.append(centroids)\n",
    "    return particles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pso(particles, ds, n_iters):\n",
    "    omega = 0.72\n",
    "    alpha = 1.49\n",
    "    velocity = np.zeros((len(particles[0]), len(ds.loc[0])))\n",
    "    global_best_fitness = float(\"-inf\")\n",
    "    global_best_error = float(\"inf\")\n",
    "    global_best_pos = []\n",
    "    personal_best_error = np.ones(len(particles))\n",
    "    personal_best_pos = np.zeros((len(particles), len(particles[0]), len(ds.loc[0])))\n",
    "    \n",
    "    for i in range(0, n_iters):\n",
    "        \n",
    "        quantization_errors = []\n",
    "        \n",
    "        for particle in particles:\n",
    "            \n",
    "            cluster_0 = []\n",
    "            cluster_1 = []\n",
    "            \n",
    "            for datapoint in ds:\n",
    "                if type(datapoint) == str:\n",
    "                    break \n",
    "#                 print(particle[0])\n",
    "                dis0 = np.linalg.norm(datapoint - particle[0])\n",
    "                dis1 = np.linalg.norm(datapoint - particle[1])\n",
    "                \n",
    "                # assigning datapoints to clusters\n",
    "                if dis0 < dis1:\n",
    "                    cluster_0.append(datapoint)\n",
    "                else:\n",
    "                    cluster_1.append(datapoint)\n",
    "                    \n",
    "            clusters = [cluster_0, cluster_1]\n",
    "            error = quantization_error(clusters, particle)        \n",
    "            quantization_errors.append(error)\n",
    "            \n",
    "        for idx, q_error in enumerate(quantization_errors):\n",
    "            \n",
    "            if q_error < personal_best_error[idx]:\n",
    "                personal_best_error[idx] = q_error\n",
    "                personal_best_pos[idx] = particles[idx]\n",
    "            \n",
    "            if personal_best_error[idx] < global_best_error:\n",
    "                global_best_error = personal_best_error[idx]\n",
    "                global_best_pos = particles[idx]\n",
    "        \n",
    "        for idx, particle in enumerate(particles):\n",
    "            \n",
    "            r1 = np.random.uniform(0, 1)\n",
    "            r2 = np.random.uniform(0, 1)\n",
    "            \n",
    "            inertia = np.multiply(omega, velocity)\n",
    "        \n",
    "            personal_infl = np.multiply((alpha * r1), (np.subtract(personal_best_pos[idx], particle)))\n",
    "            \n",
    "            social_infl = np.multiply((alpha * r2), (np.subtract(global_best_pos, particle)))\n",
    "            \n",
    "            velocity = inertia + personal_infl + social_infl\n",
    "            \n",
    "            particles[idx] = np.add(particle, velocity)\n",
    "            \n",
    "    return global_best_error, global_best_pos, quantization_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-3-cd02c6ceb50b>:7: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  quant_error += euclidean_sum/len(cluster)\n"
     ]
    }
   ],
   "source": [
    "particles = generate_particles(10, 2, artificial_ds)\n",
    "\n",
    "global_best_error, global_best_pos, quantization_error = pso(particles, artificial_ds, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.5521333459016524, -0.008145694638759693], [0.06834176329479891, -0.8224377134864265]]\n"
     ]
    }
   ],
   "source": [
    "print(global_best_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]\n"
     ]
    }
   ],
   "source": [
    "print(quantization_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ACO is an algorithm that is based on the foraging behaviour of ants. More specificaly its a probablistic algorithm that can be used to for path finding. The ants are able to solve challeging path finding problems with the use of pheromone. They are able to do this by basing their routing on the concentration of pheromone. Picking the paths with a higher concentration of pheromones more often. This can represented in a graph where the nodes are solution components and the edge weights are pheromone values. Without a tabu list ants will make cycles. This will occur almost definitely in the bottom part since there are a lot of cycles possible, which will result in a lot of pheromone accumulation. Thus resuluting in a unoptimal solution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
