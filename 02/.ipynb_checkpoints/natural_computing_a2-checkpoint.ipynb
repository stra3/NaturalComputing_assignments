{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural computing - Assignment 2\n",
    "\n",
    "#### Jelle Arts (s1010317), Ruben Geurtjens (s1006223), Lotte Willems (s1009251)\n",
    "\n",
    "\n",
    "The link to our git where you can find the notebook:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1\n",
    "\n",
    "\n",
    "* Fitness particle $x_1$: $f(x_1) = \\sum_{i=1}^{2} \\space (--400) \\cdot \\sin(\\sqrt|-400|) = 730.356$\n",
    "\n",
    "    Fitness particle $x_2$: $f(x_2) = \\sum_{i=1}^{2} \\space (--410) \\cdot \\sin(\\sqrt|-410|) = 807.915$ \n",
    "    \n",
    "    Fitness particle $x_3$: $f(x_3) = \\sum_{i=1}^{2} \\space (--415) \\cdot \\sin(\\sqrt|-415|) = 829.012$\n",
    "    \n",
    "\n",
    "* Particle updates:\n",
    "    $v_i = \\omega v_i + \\alpha_1 r_1 (x_{i}^{*} - x_i) + \\alpha_2 r_2(x^{*} - x_i)$\n",
    "    \n",
    "    **For $\\omega = 2$:**\n",
    "\n",
    "    Velocity particle $x_1$: $v_1 = 2 \\cdot -50 + 1 \\cdot 0.5 (-400 - -400) + 1 \\cdot 0.5 (-415 - -400) = -107.5 $\n",
    "    \n",
    "    New position particle $x_1$: $(-400, -400) + (-107.5, -107,5) = (-507.5, -507.5)$\n",
    "    \n",
    "    Fitness particle $x_1$: $f(x_1) = \\sum_{i=1}^{2} \\space (--507.5) \\cdot \\sin(\\sqrt|-507.5|) = -518.896$\n",
    "    \n",
    "    Velocity particle $x_2$: $v_2 = 2 \\cdot -50 + 1 \\cdot 0.5 (-410 - -410) + 1 \\cdot 0.5 (-415 - -410) = -102.5 $\n",
    "    \n",
    "    New position particle $x_2$: $(-410, -410) + (-102.5, -102.5) = (-512.5, -512.5)$ \n",
    "    \n",
    "    Fitness particle $x_2$: $f(x_2) = \\sum_{i=1}^{2} \\space (--512.5) \\cdot \\sin(\\sqrt|-512.5|) = -618.122 $\n",
    "    \n",
    "    Velocity particle $x_3$: $v_3 = 2 \\cdot -50 + 1 \\cdot 0.5 (-415 - -415) + 1 \\cdot 0.5 (-415 - -415) = -100$\n",
    "    \n",
    "    New position particle $x_3$: $(-415, 415) + (-100, -100) = (-515, -515)$\n",
    "    \n",
    "    Fitness particle $x_3$: $f(x_3) = \\sum_{i=1}^{2} \\space (--515) \\cdot \\sin(\\sqrt|-515|) = -665.482$\n",
    "    \n",
    "    **For $\\omega = 0.5$:**\n",
    "    \n",
    "    Velocity particle $x_1$: $v_1 = 0.5 \\cdot -50 + 1 \\cdot 0.5 (-400 - -400) + 1 \\cdot 0.5 (-415 - -400) = -32.5 $\n",
    "    \n",
    "    New position particle $x_1$: $(-400, -400) + (-32.5, -32.5) = (-432.5, -432.5)$\n",
    "    \n",
    "    Fitness particle $x_1$: $f(x_1) = \\sum_{i=1}^{2} \\space (--432.5) \\cdot \\sin(\\sqrt|-432.5|) = 804.482$\n",
    "    \n",
    "    Velocity particle $x_2$: $v_2 = 0.5 \\cdot -50 + 1 \\cdot 0.5 (-410 - -410) + 1 \\cdot 0.5 (-415 - -410) = -27.5 $\n",
    "    \n",
    "    New position particle $x_2$: $(-410, -410) + (-27.5, -27.5) = (-437.5, -437.5)$ \n",
    "    \n",
    "    Fitness particle $x_2$: $f(x_2) = \\sum_{i=1}^{2} \\space (--437.5) \\cdot \\sin(\\sqrt|-437.5|) = 796.495 $\n",
    "    \n",
    "    Velocity particle $x_3$: $v_3 = 0.5 \\cdot -50 + 1 \\cdot 0.5 (-415 - -415) + 1 \\cdot 0.5 (-415 - -415) = -25$\n",
    "    \n",
    "    New position particle $x_3$: $(-415, 415) + (-25, -25) = (-440, -440)$\n",
    "    \n",
    "    Fitness particle $x_3$: $f(x_3) = \\sum_{i=1}^{2} \\space (--440) \\cdot \\sin(\\sqrt|-440|) = 747.530$\n",
    "    \n",
    "    **For $\\omega = 0.1$:**\n",
    "    \n",
    "    Velocity particle $x_1$: $v_1 = 0.1 \\cdot -50 + 1 \\cdot 0.5 (-400 - -400) + 1 \\cdot 0.5 (-415 - -400) = -12.5 $\n",
    "    \n",
    "    New position particle $x_1$: $(-400, -400) + (-12.5, -12.5) = (-412.5, -412.5)$\n",
    "    \n",
    "    Fitness particle $x_1$: $f(x_1) = \\sum_{i=1}^{2} \\space (--412.5) \\cdot \\sin(\\sqrt|-412.5|) = 819.991$\n",
    "    \n",
    "    Velocity particle $x_2$: $v_2 = 0.1 \\cdot -50 + 1 \\cdot 0.5 (-410 - -410) + 1 \\cdot 0.5 (-415 - -410) = -7.5 $\n",
    "    \n",
    "    New position particle $x_2$: $(-410, -410) + (-7.5, -7.5) = (-417.5, -417.5)$ \n",
    "    \n",
    "    Fitness particle $x_2$: $f(x_2) = \\sum_{i=1}^{2} \\space (--417.5) \\cdot \\sin(\\sqrt|-417.5|) = 834.935 $\n",
    "    \n",
    "    Velocity particle $x_3$: $v_3 = 0.1 \\cdot -50 + 1 \\cdot 0.5 (-415 - -415) + 1 \\cdot 0.5 (-415 - -415) = -5$\n",
    "    \n",
    "    New position particle $x_3$: $(-415, 415) + (-5, -5) = (-420, -420)$\n",
    "    \n",
    "    Fitness particle $x_3$: $f(x_3) = \\sum_{i=1}^{2} \\space (--420) \\cdot \\sin(\\sqrt|-420|) = 837.729$\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "* Parameter $\\omega$ is the factor that determines how much the velocity of the previous position of the particle will contribute to the next velocity. From the lecture, the parameter influences the convergence of the swarm. With $\\omega > 1$, velocities increase over time causing divergent behaviour causing particles failing to change direction and restraining them from moving back towards promising areas. With $\\omega < 1$, the particles can decelerate until the velocities reach zero.\n",
    "\n",
    "* An advantage of a high $\\omega$ is that it allows the particles to move faster to the search space allowing them to explore and find a better solution. A disadvantage is that there is less exploitation, and it might therefore miss the global optimum.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2\n",
    "\n",
    "With an $\\omega < 1$, the velocity will eventually reach 0. At the start, the velocity is pointing away from the optimum. However, since the velocity decreases each step, it will move back, towards the optimum, before it reaches 0. If the particle reaches the optimum before the velocity reaches zero, the algorithm will terminate at the optimum point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.cluster import KMeans\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantization_error(clusters, centroids):\n",
    "    n_c = len(clusters)\n",
    "    \n",
    "    quant_error = 0.0\n",
    "    for cluster, centroid in zip(clusters, centroids):\n",
    "        euclidean_sum = np.sum([np.linalg.norm(x - centroid) for x in cluster])\n",
    "        if len(cluster) == 0:\n",
    "            quant_error += euclidean_sum\n",
    "        else:\n",
    "            quant_error += euclidean_sum/len(cluster)\n",
    "    \n",
    "    quant_error /= n_c\n",
    "    \n",
    "    return quant_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Artificial dataset 1 (from paper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "z1 = np.random.uniform(-1, 1, 400)\n",
    "z2 = np.random.uniform(-1, 1, 400)\n",
    "label = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(0, 400):\n",
    "#     if (z1[i] >= 0.7) or ((z1[i] <= 0.3) and (z2[i] >= (-0.2 - z1[i]))):\n",
    "#         label.append(1)\n",
    "        \n",
    "#     else:\n",
    "#         label.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>z1</th>\n",
       "      <th>z2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.498824</td>\n",
       "      <td>0.545144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.353202</td>\n",
       "      <td>-0.543937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.177680</td>\n",
       "      <td>0.315772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.015122</td>\n",
       "      <td>0.179244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.621533</td>\n",
       "      <td>0.416680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395</th>\n",
       "      <td>0.548766</td>\n",
       "      <td>-0.895523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>-0.881531</td>\n",
       "      <td>0.112011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397</th>\n",
       "      <td>-0.494282</td>\n",
       "      <td>0.004922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>398</th>\n",
       "      <td>-0.834686</td>\n",
       "      <td>-0.588398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399</th>\n",
       "      <td>-0.912372</td>\n",
       "      <td>-0.540254</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>400 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           z1        z2\n",
       "0    0.498824  0.545144\n",
       "1    0.353202 -0.543937\n",
       "2   -0.177680  0.315772\n",
       "3    0.015122  0.179244\n",
       "4    0.621533  0.416680\n",
       "..        ...       ...\n",
       "395  0.548766 -0.895523\n",
       "396 -0.881531  0.112011\n",
       "397 -0.494282  0.004922\n",
       "398 -0.834686 -0.588398\n",
       "399 -0.912372 -0.540254\n",
       "\n",
       "[400 rows x 2 columns]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "artificial_ds = pd.DataFrame({\"z1\" : z1, \"z2\" : z2})\n",
    "artificial_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Iris dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)\n",
      "0                  5.1               3.5                1.4               0.2\n",
      "1                  4.9               3.0                1.4               0.2\n",
      "2                  4.7               3.2                1.3               0.2\n",
      "3                  4.6               3.1                1.5               0.2\n",
      "4                  5.0               3.6                1.4               0.2\n",
      "..                 ...               ...                ...               ...\n",
      "145                6.7               3.0                5.2               2.3\n",
      "146                6.3               2.5                5.0               1.9\n",
      "147                6.5               3.0                5.2               2.0\n",
      "148                6.2               3.4                5.4               2.3\n",
      "149                5.9               3.0                5.1               1.8\n",
      "\n",
      "[150 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "iris_data = datasets.load_iris()\n",
    "\n",
    "iris_ds = pd.DataFrame(iris_data.data, columns = iris_data.feature_names)\n",
    "\n",
    "print(iris_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K Means artificial dataset 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_art = KMeans(n_clusters = 2).fit(artificial_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_clusters_art = kmeans_art.predict(artificial_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clusters(pred, ds):\n",
    "    \n",
    "    cluster_0 = []\n",
    "    cluster_1 = []\n",
    "\n",
    "    for i in range(0, len(ds)):\n",
    "        if pred[i] == 0:\n",
    "            cluster_0.append(ds.loc[i])\n",
    "\n",
    "        if pred[i] == 1:\n",
    "            cluster_1.append(ds.loc[i])\n",
    "    return [cluster_0, cluster_1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5781997366274441"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clusters_art = get_clusters(predicted_clusters_art, artificial_ds)\n",
    "quantization_error(clusters_art, kmeans_art.cluster_centers_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K Means iris datset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_iris = KMeans(n_clusters = 3).fit(iris_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_clusters_iris = kmeans_iris.predict(iris_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clusters_iris(pred, ds):\n",
    "    \n",
    "    cluster_0 = []\n",
    "    cluster_1 = []\n",
    "    cluster_2 = []\n",
    "\n",
    "    for i in range(0, len(ds)):\n",
    "        if pred[i] == 0:\n",
    "            cluster_0.append(ds.loc[i])\n",
    "\n",
    "        if pred[i] == 1:\n",
    "            cluster_1.append(ds.loc[i])\n",
    "            \n",
    "        if pred[i] == 2:\n",
    "            cluster_2.append(ds.loc[i])\n",
    "            \n",
    "    return [cluster_0, cluster_1, cluster_2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6465653848597094"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clusters_iris = get_clusters_iris(predicted_clusters_iris, iris_ds)\n",
    "quantization_error(clusters_iris, kmeans_iris.cluster_centers_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PSO algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_particles_ds1(n_particles, n_centroids, ds):\n",
    "    particles = []\n",
    "    for i in range(0, n_particles):\n",
    "        centroids = []\n",
    "        for j in range(0, n_centroids):\n",
    "            centroids.append([ds.iloc[np.random.randint(0, len(ds))][\"z1\"], ds.iloc[np.random.randint(0, len(ds))][\"z2\"]])\n",
    "        particles.append(centroids)\n",
    "    return particles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pso_ds1(particles, ds, n_iters):\n",
    "    ds = ds.to_numpy()\n",
    "    omega = 0.72\n",
    "    alpha = 1.49\n",
    "    velocity = np.zeros((len(particles[0]), len(ds[0])))\n",
    "    global_best_fitness = float(\"-inf\")\n",
    "    global_best_error = float(\"inf\")\n",
    "    global_best_pos = []\n",
    "    personal_best_error = np.ones(len(particles))\n",
    "    personal_best_pos = np.zeros((len(particles), len(particles[0]), len(ds[0])))\n",
    "    \n",
    "    min_quant_error = []\n",
    "    \n",
    "    for i in range(0, n_iters):\n",
    "        \n",
    "        quantization_errors = []\n",
    "        \n",
    "        for particle in particles:\n",
    "    \n",
    "            cluster_0 = []\n",
    "            cluster_1 = []\n",
    "                        \n",
    "            for datapoint in ds:\n",
    "                dis0 = np.linalg.norm(datapoint - particle[0])\n",
    "                dis1 = np.linalg.norm(datapoint - particle[1])\n",
    "\n",
    "                # assigning datapoints to clusters\n",
    "                if dis0 < dis1:\n",
    "                    cluster_0.append(datapoint)\n",
    "                else:\n",
    "                    cluster_1.append(datapoint)\n",
    "                    \n",
    "            clusters = [cluster_0, cluster_1]\n",
    "            error = quantization_error(clusters, particle)        \n",
    "            quantization_errors.append(error)\n",
    "            \n",
    "        min_quant_error.append(np.min(quantization_errors))\n",
    "        \n",
    "        for idx, q_error in enumerate(quantization_errors):\n",
    "            \n",
    "            if q_error < personal_best_error[idx]:\n",
    "                personal_best_error[idx] = q_error\n",
    "                personal_best_pos[idx] = particles[idx]\n",
    "            \n",
    "            if personal_best_error[idx] < global_best_error:\n",
    "                global_best_error = personal_best_error[idx]\n",
    "                global_best_pos = particles[idx]\n",
    "                \n",
    "        \n",
    "        for idx, particle in enumerate(particles):\n",
    "            \n",
    "            r1 = np.random.uniform(0, 1)\n",
    "            r2 = np.random.uniform(0, 1)\n",
    "            \n",
    "            inertia = omega * velocity\n",
    "        \n",
    "            personal_infl = np.multiply((alpha * r1), (np.subtract(personal_best_pos[idx], particle)))\n",
    "            \n",
    "            social_infl = np.multiply((alpha * r2), (np.subtract(global_best_pos, particle)))\n",
    "            \n",
    "            velocity = inertia + personal_infl + social_infl\n",
    "            \n",
    "            particles[idx] = np.add(particle, velocity)\n",
    "            \n",
    "    return global_best_error, global_best_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "particles_ds1 = generate_particles(10, 2, artificial_ds)\n",
    "\n",
    "global_best_error_ds1, global_best_pos_ds1 = pso_ds1(particles_ds1, artificial_ds, 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3789282099178677\n"
     ]
    }
   ],
   "source": [
    "print(global_best_error_ds1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.6225589971348913, 0.410986942898942, 0.3789282099178677, 0.4524799336291923, 2.7679550503408126, 2.726496276036595, 5.083180467828502, 15.432118071323362, 34.19023930813374, 197.60430402574107, 503.20724112104887, 269.88178448126615, 2973.034554692164, 7051.388757299066, 21245.450678967914]\n"
     ]
    }
   ],
   "source": [
    "print(min_quant_error_ds1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_particles_iris(n_particles, n_centroids, ds):\n",
    "    particles = []\n",
    "    for i in range(0, n_particles):\n",
    "        centroids = []\n",
    "        for j in range(0, n_centroids):\n",
    "            centroids.append([ds.iloc[np.random.randint(0, len(ds))][\"sepal length (cm)\"], \n",
    "                              ds.iloc[np.random.randint(0, len(ds))][\"sepal width (cm)\"],\n",
    "                              ds.iloc[np.random.randint(0, len(ds))][\"petal length (cm)\"],\n",
    "                              ds.iloc[np.random.randint(0, len(ds))][\"petal width (cm)\"],\n",
    "                             ])\n",
    "        particles.append(centroids)\n",
    "    return particles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pso_iris(particles, ds, n_iters):\n",
    "    ds = ds.to_numpy()\n",
    "    omega = 0.72\n",
    "    alpha = 1.49\n",
    "    velocity = np.zeros((len(particles[0]), len(ds[0])))\n",
    "    global_best_fitness = float(\"-inf\")\n",
    "    global_best_error = 10000\n",
    "    global_best_pos = np.zeros((len(particles), len(particles[0]), len(ds[0])))\n",
    "    personal_best_error = np.full((len(particles)), np.inf)\n",
    "    personal_best_pos = np.zeros((len(particles), len(particles[0]), len(ds[0])))\n",
    "    \n",
    "    min_quant_error = []\n",
    "    \n",
    "    for i in range(0, n_iters):\n",
    "        \n",
    "        quantization_errors = []\n",
    "        \n",
    "        for particle in particles:\n",
    "    \n",
    "            cluster_0 = []\n",
    "            cluster_1 = []\n",
    "            cluster_2 = []\n",
    "                        \n",
    "            for datapoint in ds:\n",
    "                dis0 = np.linalg.norm(datapoint - particle[0])\n",
    "                dis1 = np.linalg.norm(datapoint - particle[1])\n",
    "                dis2 = np.linalg.norm(datapoint - particle[2])\n",
    "\n",
    "                # assigning datapoints to clusters\n",
    "                if dis0 < dis1 and dis0 < dis2:\n",
    "                    cluster_0.append(datapoint)\n",
    "                    \n",
    "                if dis1 < dis0 and dis1 < dis2:\n",
    "                    cluster_1.append(datapoint)\n",
    "                    \n",
    "                if dis2 < dis0 and dis2 < dis1:\n",
    "                    cluster_2.append(datapoint)\n",
    "                    \n",
    "            clusters = [cluster_0, cluster_1, cluster_2]\n",
    "            error = quantization_error(clusters, particle) \n",
    "            print(error)\n",
    "            quantization_errors.append(error)\n",
    "            \n",
    "        min_quant_error.append(np.min(quantization_errors))\n",
    "        \n",
    "        for idx, q_error in enumerate(quantization_errors):\n",
    "            \n",
    "            if q_error < personal_best_error[idx]:\n",
    "                personal_best_error[idx] = q_error\n",
    "                personal_best_pos[idx] = particles[idx]\n",
    "            \n",
    "            if personal_best_error[idx] < global_best_error:\n",
    "                global_best_error = personal_best_error[idx]\n",
    "                global_best_pos = np.array(particles[idx])\n",
    "                \n",
    "        \n",
    "        for idx, particle in enumerate(particles):\n",
    "            \n",
    "            particle = np.array(particle)\n",
    "            \n",
    "            \n",
    "            r1 = np.random.uniform(0, 1)\n",
    "            r2 = np.random.uniform(0, 1)\n",
    "            \n",
    "            inertia = omega * velocity\n",
    "        \n",
    "#             personal_infl = np.multiply((alpha * r1), (np.subtract(personal_best_pos[idx], particle)))\n",
    "    \n",
    "            personal_infl = np.multiply((alpha * r1), (personal_best_pos[idx] - particle))\n",
    "        \n",
    "#             social_infl = np.multiply((alpha * r2), (np.subtract(global_best_pos, particle)))\n",
    "            social_infl = np.multiply((alpha * r2), (global_best_pos - particle))\n",
    "        \n",
    "            velocity = inertia + personal_infl + social_infl\n",
    "            \n",
    "            particles[idx] = np.add(particle, velocity)\n",
    "            \n",
    "    return global_best_error, global_best_pos, min_quant_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.2933828695475953\n",
      "2.4766487436337115\n",
      "2.314511257992868\n",
      "0.6286772033740538\n",
      "1.8655191464465475\n",
      "1.727754170973087\n",
      "1.8409301799061388\n",
      "1.4111196438927394\n",
      "1.4684026569917166\n",
      "1.8084028925373818\n",
      "0.713263435552817\n",
      "1.7627325212338036\n",
      "2.8294922571468226\n",
      "1.1190073941326957\n",
      "2.105816066773027\n",
      "1.7587593331102402\n",
      "1.6459088567387798\n",
      "2.2634274991757057\n",
      "1.6807897311667348\n",
      "2.380934404498472\n",
      "2.450061771829572\n",
      "1.7485431853702025\n",
      "1.227573854775711\n",
      "1.3277554164005936\n",
      "2.445135102550693\n",
      "3.494857884115302\n",
      "2.353202025271504\n",
      "7.114862506189705\n",
      "8.181279723102707\n",
      "6.313145594711748\n",
      "6.781158154940581\n",
      "4.693320764241593\n",
      "4.11644960193327\n",
      "3.1439803934263035\n",
      "2.5242212691533417\n",
      "2.2274240927581306\n",
      "3.8764398192875853\n",
      "4.7935804356775895\n",
      "6.875836434332167\n",
      "12.484047288541397\n",
      "15.538615753970719\n",
      "12.784090220596548\n",
      "13.733109491001139\n",
      "11.537867830679195\n",
      "10.80200581767462\n",
      "9.023847125038893\n",
      "11.896014123467884\n",
      "9.240961484265533\n",
      "8.998304970814702\n",
      "5.07226946501922\n",
      "15.87910317415443\n",
      "27.002529775829988\n",
      "26.246647748816937\n",
      "32.521540997233096\n",
      "46.89789432577791\n",
      "34.04539291521382\n",
      "31.05716230731144\n",
      "28.813025945982677\n",
      "20.648402873699613\n",
      "10.460916026248032\n",
      "10.855193554284115\n",
      "61.02140726049856\n",
      "82.32079619715475\n",
      "79.71737815920166\n",
      "127.10834708796688\n",
      "184.3359102310063\n",
      "130.9529939642538\n",
      "147.929100088895\n",
      "142.9162913701392\n",
      "128.9302759936895\n",
      "105.00735027994482\n",
      "42.524436071710376\n",
      "193.47330641495816\n",
      "158.96329658830402\n",
      "75.81408614853257\n",
      "299.19181431998726\n",
      "391.48312012776614\n",
      "352.2441441809962\n",
      "475.23340763341344\n",
      "421.73926593339826\n",
      "373.3624761224442\n",
      "349.3533250124496\n",
      "265.7301227157371\n",
      "73.76700475023881\n",
      "218.09181386560113\n",
      "467.14076650979996\n",
      "843.4456914953547\n",
      "831.4485609871685\n",
      "1076.1623963402628\n",
      "1480.1450579672364\n",
      "1648.570284527541\n",
      "1619.1865604271725\n",
      "1273.5996537772385\n",
      "1103.7217379387096\n",
      "678.563870445835\n",
      "321.8104282085472\n",
      "222.92278824188404\n",
      "1519.949474511445\n",
      "1369.6311913534776\n",
      "2462.390576668175\n",
      "5463.587661435814\n",
      "5775.249239850775\n",
      "5701.277879374197\n",
      "5986.339401286197\n",
      "5500.599300234699\n",
      "4503.861630506303\n",
      "3449.2511316135938\n",
      "1449.6299395295584\n",
      "1233.988439826655\n",
      "1955.3829256757465\n",
      "4454.388976202013\n",
      "11791.006389660879\n",
      "14027.280418144393\n",
      "9404.685680403089\n",
      "13002.984467293243\n",
      "17438.08890411803\n",
      "17931.1043598005\n",
      "15447.740404498205\n",
      "12465.513569831155\n",
      "10155.72707341501\n",
      "3005.662563331165\n",
      "7962.4164148226455\n",
      "20454.829274751082\n",
      "41393.93944340853\n",
      "41642.945351191476\n",
      "43273.89160296383\n",
      "67490.19475319098\n",
      "53386.838141928696\n",
      "56764.099466603286\n",
      "48236.29858723024\n",
      "44784.690413945376\n",
      "33355.42807881194\n",
      "7317.405973893766\n",
      "84179.63888973727\n",
      "138549.14710204347\n",
      "162560.0366190742\n",
      "203741.75932551615\n",
      "257893.12493521438\n",
      "206965.925672929\n",
      "204406.21641653552\n",
      "212066.42522614638\n",
      "202952.96620368675\n",
      "169958.6764504733\n",
      "107876.9878853933\n",
      "29675.80101888665\n",
      "215037.5427186533\n",
      "495971.4846715341\n",
      "896009.8301872312\n",
      "884298.6635165811\n",
      "827022.1093314994\n"
     ]
    }
   ],
   "source": [
    "particles_iris = generate_particles_iris(10, 3, iris_ds)\n",
    "\n",
    "global_best_error_iris, global_best_pos_iris, min_q_error_iris = pso_iris(particles_iris, iris_ds, 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6286772033740538\n"
     ]
    }
   ],
   "source": [
    "print(global_best_error_iris)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5.  3.5 1.4 2.3]\n",
      " [5.5 3.5 1.6 0.2]\n",
      " [6.2 3.  4.5 2. ]]\n"
     ]
    }
   ],
   "source": [
    "print(global_best_pos_iris)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.6286772033740538, 0.713263435552817, 1.227573854775711, 2.2274240927581306, 5.07226946501922, 10.460916026248032, 10.855193554284115, 42.524436071710376, 73.76700475023881, 222.92278824188404, 1233.988439826655, 4454.388976202013, 3005.662563331165, 7317.405973893766, 29675.80101888665]\n"
     ]
    }
   ],
   "source": [
    "print(min_q_error_iris)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ACO (ant colony optimization) is an algorithm that is based on the foraging behaviour of ants. More specifically, it is a probablistic algorithm that can be used for path finding. Ants are able to solve challeging path finding problems with the use of pheromones. They are able to do this by basing their routing on the concentration of pheromone, picking the paths with a higher concentration of pheromones more often. This can be represented in a graph where the nodes are solution components and the edge weights are pheromone values. The goal is to reach the destination node following the shortest path in the graph for which the ACO algorithm can be used. In an ACO without a tabu list, ants will make cycles in the graph. In the given figure, this will occur almost definitely in the bottom part since there are a lot of cycles possible, which will result in a lot of pheromone accumulation. Thus, following cycles will result in a unoptimal solution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
