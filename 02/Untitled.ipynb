{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.cluster import KMeans\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "z1=np.random.uniform(-1,1,400)\n",
    "z2=np.random.uniform(-1,1,400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "artificial_df=pd.DataFrame({\"Z1\" : z1,\"Z2\" : z2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans1 = KMeans(n_clusters=2, random_state=0).fit(artificial_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_artificial_df=kmeans1.predict(artificial_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "centroid1_A_df=kmeans1.cluster_centers_[0]\n",
    "centroid2_A_df=kmeans1.cluster_centers_[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster1_A=[]\n",
    "cluster2_A=[]\n",
    "for i in range(0,len(artificial_df)):\n",
    "    if prediction_artificial_df[i]==0:\n",
    "        \n",
    "        cluster1_A.append(artificial_df.loc[i])\n",
    "        \n",
    "    if prediction_artificial_df[i]==1:\n",
    "        cluster2_A.append(artificial_df.loc[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantization_error2(cluster1,cluster2,centroid1,centroid2):\n",
    "            \n",
    "    distance1=0\n",
    "    distance2=0\n",
    "\n",
    "\n",
    "    for i in range(0,len(cluster1)):\n",
    "\n",
    "        dist1=scipy.spatial.distance.euclidean(np.asarray(cluster1[i]), centroid1)\n",
    "        distance1=distance1+dist1\n",
    "    \n",
    "    if len(cluster1)>0:\n",
    "        distance1=distance1/len(cluster1)\n",
    "    else:\n",
    "        distance1=0\n",
    "\n",
    "    for i in range(0,len(cluster2)):\n",
    "\n",
    "        dist2=scipy.spatial.distance.euclidean(np.asarray(cluster2[i]), centroid2)\n",
    "        distance2=distance2+dist2\n",
    "    \n",
    "    if len(cluster2)>0:\n",
    "        distance2=distance2/len(cluster2)\n",
    "    else:\n",
    "        distance2=0\n",
    "\n",
    "\n",
    "    QE=(distance1+distance2)/2\n",
    "    ##print(\"Quantization error: \",QE)\n",
    "    return QE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The quantization error for the KMEANS algorithm for the artificial dataset is   0.5666574240638876\n"
     ]
    }
   ],
   "source": [
    "print (\"The quantization error for the KMEANS algorithm for the artificial dataset is  \", quantization_error2(cluster1_A,cluster2_A,centroid1_A_df,centroid2_A_df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "particles_list=[]\n",
    "for i in range(0,10):\n",
    "    particles=np.random.uniform(-1,1,4).reshape(2,2)\n",
    "    particles_list.append(particles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After  9  iteration , the global best error is:  0.40708035270694104 best  swarn has positions:  [[1.3130555  2.27214181]\n",
      " [0.1950484  0.31202336]]\n",
      "[0.6049142047129293, 0.40708035270694104, 0.42496899891610723, 0.47749938471237924, 1.181324312959327, 3.733493605510646, 5.066579945080508, 17.300274202699264, 49.09387909738464, 172.65453170890152]\n"
     ]
    }
   ],
   "source": [
    "personal_best_error=np.ones(10)\n",
    "global_best_position=[]\n",
    "global_best_error=1\n",
    "personal_best_position=np.zeros(40).reshape(10,2,2)\n",
    "\n",
    "alpha=1.49618\n",
    "w=0.7298\n",
    "velocity=np.zeros(4).reshape(2,2) \n",
    "min_quant_error=[]\n",
    "##for each iteration\n",
    "for iteration in range(0,10):\n",
    "    \n",
    "    quantization_error=[] \n",
    "    count=0\n",
    "    \n",
    "    ##for each particles \n",
    "    for i in range(0,len(particles_list)):\n",
    "\n",
    "        \n",
    "        current_particle=particles_list[i]\n",
    "        cluster1=[]\n",
    "        cluster2=[]\n",
    "        \n",
    "       ##for each data point in the Artificial dataset\n",
    "        for a in range(0,len(artificial_df)):\n",
    "            \n",
    "            \n",
    "            ##we measure the euclidean distance from each centroid\n",
    "            dist1=scipy.spatial.distance.euclidean(np.asarray(artificial_df.loc[a]), current_particle[0])\n",
    "            dist2=scipy.spatial.distance.euclidean(np.asarray(artificial_df.loc[a]), current_particle[1])\n",
    "\n",
    "            ##we assign each datapoint to a cluster based on the closest centroid\n",
    "            if dist1<dist2:\n",
    "                cluster1.append(artificial_df.loc[a])\n",
    "\n",
    "\n",
    "            elif dist1>dist2:\n",
    "                cluster2.append(artificial_df.loc[a])\n",
    "\n",
    "\n",
    "        \n",
    "        ##Once we have our cluster we compute the quantization error and we store it\n",
    "        ## we will have a list with all our quantization error\n",
    "        quantization_error.append(quantization_error2(cluster1,cluster2,current_particle[0],current_particle[1]))\n",
    "#         print(quantization_error)\n",
    "\n",
    "    min_quant_error.append(min(quantization_error))\n",
    "    \n",
    "    ##Then for each quantization error in the list\n",
    "    for z in range(0,len(quantization_error)):\n",
    "        count=count+1    \n",
    "        \n",
    "        ##if the quantization error of the current iteration is smaller than the personal best , we store it as new personal best\n",
    "        if quantization_error[z]<personal_best_error[z]:\n",
    "            personal_best_error[z]=quantization_error[z]\n",
    "            ##and we store the corresponding particle as new best position\n",
    "            personal_best_position[z]=particles_list[z]\n",
    "\n",
    "        ##we also update the best global error if we find a new best \n",
    "        if personal_best_error[z]<global_best_error:\n",
    "            global_best_error=personal_best_error[z]\n",
    "            global_best_position=particles_list[z]\n",
    "            \n",
    "    ###print (\"Particle list BEFORE, \", particles_list)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    ## then for each particle we update the position of the centroids based on the formula\n",
    "    \n",
    "    for i in range(0,len(particles_list)):\n",
    "        \n",
    "        ##I've disassembled the formula for clarity.\n",
    "\n",
    "        \n",
    "\n",
    "        ##we set the random r\n",
    "        r1=np.random.uniform(0,1,1)\n",
    "        r2=np.random.uniform(0,1,1)\n",
    "\n",
    "        ##we compute the first term of the formula\n",
    "        first_term=np.multiply(w,velocity)\n",
    "        \n",
    "        ##we compute the two multiplication of the alpha and the r\n",
    "        alphaR1=alpha*r1\n",
    "        alphaR2=alpha*r2\n",
    "        \n",
    "        #we copmute the 2 subtractions\n",
    "        first_subtraction=np.subtract(personal_best_position[i],particles_list[i])\n",
    "        second_subtraction=np.subtract(global_best_position,particles_list[i])\n",
    "\n",
    "        #we compute the second term\n",
    "        second_term=np.multiply(alphaR1,first_subtraction)\n",
    "        \n",
    "        #we compute the second term\n",
    "        third_term=np.multiply(alphaR2,second_subtraction)\n",
    "        \n",
    "        ##we compute the velocity\n",
    "        velocity=np.add(first_term,second_term)\n",
    "        velocity=np.add(velocity,third_term)\n",
    "\n",
    "        #we update the position of the centroids of the particles\n",
    "        particles_list[i]=np.add(particles_list[i],velocity)\n",
    "        \n",
    "    ##print (\"global best error is: \", global_best_error)\n",
    "    ##print (\"global best position: \", global_best_position)\n",
    "    ##print (\"personal best error: \", personal_best_error)\n",
    "    ##print (\"Particle list AFTER\", particles_list)\n",
    "    ##print(\"personal best position  \",personal_best_position )\n",
    "\n",
    "    \n",
    "print(\"After \",iteration,\" iteration , the global best error is: \", global_best_error, \"best  swarn has positions: \", global_best_position)\n",
    "print(min_quant_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
